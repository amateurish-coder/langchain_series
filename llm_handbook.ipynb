{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amateurish-coder/langchain_series/blob/main/langchain_handbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39a30cb-7280-4cb5-9c08-ab4ed1a7b2b4",
      "metadata": {
        "id": "a39a30cb-7280-4cb5-9c08-ab4ed1a7b2b4"
      },
      "source": [
        "# LLM handbook\n",
        "\n",
        "Following guidance from <a href='https://www.pinecone.io/learn/series/langchain/'> Pinecone's Langchain handbook.</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1qUakls_hN6R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qUakls_hN6R",
        "outputId": "c9988f04-0c1e-41fb-d239-638562d6f754"
      },
      "outputs": [],
      "source": [
        "# # if using Google Colab\n",
        "# !pip install langchain\n",
        "# !pip install huggingface_hub\n",
        "# !pip install python-dotenv\n",
        "# !pip install pypdf2\n",
        "# !pip install faiss-cpu\n",
        "# !pip install sentence_transformers\n",
        "# !pip install InstructorEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9fcd2583-d0ab-4649-a241-4526f6a3b83d",
      "metadata": {
        "id": "9fcd2583-d0ab-4649-a241-4526f6a3b83d"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import os\n",
        "import langchain\n",
        "import getpass\n",
        "from langchain import HuggingFaceHub, LLMChain\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AyRxKsE4qPR1",
      "metadata": {
        "id": "AyRxKsE4qPR1"
      },
      "source": [
        "#API KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cf146257-5014-4041-980c-0ead2c3932c3",
      "metadata": {
        "id": "cf146257-5014-4041-980c-0ead2c3932c3"
      },
      "outputs": [],
      "source": [
        "# LOCAL\n",
        "load_dotenv()\n",
        "os.environ.get('HUGGINGFACEHUB_API_TOKEN');\n",
        "\n",
        "# # COLAB\n",
        "# from google.colab import userdata\n",
        "# os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HUGGINGFACEHUB_API_TOKEN');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yeGkB8OohG93",
      "metadata": {
        "id": "yeGkB8OohG93"
      },
      "source": [
        "# Skill 1 - using prompt templates\n",
        "\n",
        "A prompt is the input to the LLM. Learning to engineer the prompt is learning how to program the LLM to do what you want it to do. The most basic prompt class from langchain is the PromptTemplate which is demonstrated below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "06c54d35-e9a2-4043-b3c3-588ac4f4a0d1",
      "metadata": {
        "id": "06c54d35-e9a2-4043-b3c3-588ac4f4a0d1"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# create template\n",
        "template = \"\"\"\n",
        "Answer the following question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# create prompt using template\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=['question']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A1rhV_L1hG94",
      "metadata": {
        "id": "A1rhV_L1hG94"
      },
      "source": [
        "The next step is to instantiate the LLM. The LLM is fetched from HuggingFaceHub, where we can specify which model we want to use and set its parameters with <a href=https://huggingface.co/docs/transformers/main_classes/text_generation>this as reference </a>. We then set up the prompt+LLM chain using langchain's LLMChain class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "03290cad-f6be-4002-b177-00220f22333a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03290cad-f6be-4002-b177-00220f22333a",
        "outputId": "f5dde425-cf9d-416b-a030-3c5d065bafcb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/danielsuarez-mash/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# instantiate llm\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id='tiiuae/falcon-7b-instruct',\n",
        "    model_kwargs={\n",
        "        'temperature':1,\n",
        "        'penalty_alpha':2,\n",
        "        'top_k':50,\n",
        "        'max_length': 1000\n",
        "    }\n",
        ")\n",
        "\n",
        "# instantiate chain\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SeVzuXAxhG96",
      "metadata": {
        "id": "SeVzuXAxhG96"
      },
      "source": [
        "Now all that's left to do is ask a question and run the chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "92bcc47b-da8a-4641-ae1d-3beb3f870a4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92bcc47b-da8a-4641-ae1d-3beb3f870a4f",
        "outputId": "2cb57096-85a4-4c3b-d333-2c20ba4f8166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Answer the following question: How many champions league titles has Real Madrid won?\n",
            "\n",
            "Answer:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Real Madrid has won 15 Champions League titles.\n"
          ]
        }
      ],
      "source": [
        "# define question\n",
        "question = \"How many champions league titles has Real Madrid won?\"\n",
        "\n",
        "# run question\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OOXGnVnRhG96",
      "metadata": {
        "id": "OOXGnVnRhG96"
      },
      "source": [
        "# Skill 2 - using chains\n",
        "\n",
        "Chains are at the core of langchain. They represent a sequence of actions. Above, we used a simple prompt + LLM chain. Let's try some more complex chains."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kc59-q-NhG97",
      "metadata": {
        "id": "kc59-q-NhG97"
      },
      "source": [
        "## Math chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ClxH-ST-hG97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClxH-ST-hG97",
        "outputId": "f950d00b-6e7e-4b49-ef74-ad8963c76a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "Calculate 5-3?\u001b[32;1m\u001b[1;3m```text\n",
            "5 - 3\n",
            "```\n",
            "...numexpr.evaluate(\"5 - 3\")...\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m2\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Answer: 2'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import LLMMathChain\n",
        "\n",
        "llm_math_chain = LLMMathChain.from_llm(llm, verbose=True)\n",
        "\n",
        "llm_math_chain.run(\"Calculate 5-3?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-WmXZ6nLhG98",
      "metadata": {
        "id": "-WmXZ6nLhG98"
      },
      "source": [
        "We can see what prompt the LLMMathChain class is using here. This is a good example of how to program an LLM for a specific purpose using prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ecbnY7jqhG98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecbnY7jqhG98",
        "outputId": "a3f37a81-3b44-41f7-8002-86172ad4e085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
            "\n",
            "Question: ${{Question with math problem.}}\n",
            "```text\n",
            "${{single line mathematical expression that solves the problem}}\n",
            "```\n",
            "...numexpr.evaluate(text)...\n",
            "```output\n",
            "${{Output of running the code}}\n",
            "```\n",
            "Answer: ${{Answer}}\n",
            "\n",
            "Begin.\n",
            "\n",
            "Question: What is 37593 * 67?\n",
            "```text\n",
            "37593 * 67\n",
            "```\n",
            "...numexpr.evaluate(\"37593 * 67\")...\n",
            "```output\n",
            "2518731\n",
            "```\n",
            "Answer: 2518731\n",
            "\n",
            "Question: 37593^(1/5)\n",
            "```text\n",
            "37593**(1/5)\n",
            "```\n",
            "...numexpr.evaluate(\"37593**(1/5)\")...\n",
            "```output\n",
            "8.222831614237718\n",
            "```\n",
            "Answer: 8.222831614237718\n",
            "\n",
            "Question: {question}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(llm_math_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rGxlC_srhG99",
      "metadata": {
        "id": "rGxlC_srhG99"
      },
      "source": [
        "## Transform chain\n",
        "\n",
        "The transform chain allows transform queries before they are fed into the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7aXq5CGLhG99",
      "metadata": {
        "id": "7aXq5CGLhG99"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# define function to transform query\n",
        "def transform_func(inputs: dict) -> dict:\n",
        "\n",
        "    question = inputs['question']\n",
        "\n",
        "    question = re.sub(' +', ' ', question)\n",
        "\n",
        "    return {'output_question': question}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "lEG14RpahG99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lEG14RpahG99",
        "outputId": "0e9243c5-b506-48a1-8036-a54b2cd8ab53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello my name is Daniel'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import TransformChain\n",
        "\n",
        "# define transform chain\n",
        "transform_chain = TransformChain(input_variables=['question'], output_variables=['output_question'], transform=transform_func)\n",
        "\n",
        "# test transform chain\n",
        "transform_chain.run('Hello   my name is     Daniel')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "JRNuu4mLhG99",
      "metadata": {
        "id": "JRNuu4mLhG99"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "zXolBaHNhG9-",
      "metadata": {
        "id": "zXolBaHNhG9-"
      },
      "outputs": [],
      "source": [
        "# create new prompt to take input as 'output_question'\n",
        "template = \"\"\"\n",
        "Answer this question: {output_question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=['output_question'])\n",
        "\n",
        "llm_chain.prompt = prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "TOzl_x6KhG9-",
      "metadata": {
        "id": "TOzl_x6KhG9-"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "sequential_chain = SequentialChain(chains=[transform_chain, llm_chain], input_variables=['question'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "dRuMuSNWhG9_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRuMuSNWhG9_",
        "outputId": "b676c693-113a-4757-bcbe-cb0c02e45d15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Answer this question: What will happen to me if I only get 4 hours sleep tonight?\n",
            "\n",
            "Answer:\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "If you only get 4 hours of sleep tonight, you will likely feel tired, groggy, and less productive throughout the day. Getting enough adequate sleep is important for your physical and mental wellbeing. It's recommended to aim for 6-8 hours of sleep most nights of the week.\n"
          ]
        }
      ],
      "source": [
        "print(sequential_chain.run(\"What     will happen     to  me if I only get 4 hours sleep tonight?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IzVk22o3tAXu",
      "metadata": {
        "id": "IzVk22o3tAXu"
      },
      "source": [
        "# Skill 3 - conversational memory\n",
        "\n",
        "In order to have a conversation, the LLM now needs two inputs - the new query and the chat history.\n",
        "\n",
        "ConversationChain is a chain which manages these two inputs with an appropriate template as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "Qq3No2kChG9_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq3No2kChG9_",
        "outputId": "3dc29aed-2b1d-42c1-ec69-969e82bb025f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation_chain = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "print(conversation_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AJ9X_UnlTNFN",
      "metadata": {
        "id": "AJ9X_UnlTNFN"
      },
      "source": [
        "## ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3q6q0qkus6Z",
      "metadata": {
        "id": "e3q6q0qkus6Z"
      },
      "source": [
        "To manage conversation history, we can use ConversationalBufferMemory which inputs the raw chat history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "noJ8pG9muDZK",
      "metadata": {
        "id": "noJ8pG9muDZK"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "\n",
        "# set memory type\n",
        "conversation_chain.memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "WCqQ53PAOZmv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCqQ53PAOZmv",
        "outputId": "204005ab-621a-48e4-e2b2-533c5f53424e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: What is the weather like today?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the weather like today?',\n",
              " 'history': '',\n",
              " 'response': ' According to my data, it is partly cloudy with a high of 72 degrees.\\nUser '}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_chain(\"What is the weather like today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "DyGNbP4xvQRw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyGNbP4xvQRw",
        "outputId": "70bd84ee-01d8-414c-bff5-5f9aa8cc4ad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is the weather like today?\n",
            "AI:  According to my data, it is partly cloudy with a high of 72 degrees.\n",
            "User \n",
            "Human: What was my previous question?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What was my previous question?',\n",
              " 'history': 'Human: What is the weather like today?\\nAI:  According to my data, it is partly cloudy with a high of 72 degrees.\\nUser ',\n",
              " 'response': ' \\nWhat is the weather like today?\\nUser '}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_chain(\"What was my previous question?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_cI5kVPyTL7-",
      "metadata": {
        "id": "_cI5kVPyTL7-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "T4NiJP9uTQGt",
      "metadata": {
        "id": "T4NiJP9uTQGt"
      },
      "source": [
        "## ConversationSummaryMemory\n",
        "\n",
        "LLMs have token limits, meaning at some point it won't be feasible to keep feeding the entire chat history as an input. As an alternative, we can summarise the chat history using another LLM of our choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "y0DzHCo4sDha",
      "metadata": {
        "id": "y0DzHCo4sDha"
      },
      "outputs": [],
      "source": [
        "from langchain.memory.summary import ConversationSummaryMemory\n",
        "\n",
        "# change memory type\n",
        "conversation_chain.memory = ConversationSummaryMemory(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "iDRjcCoVTpnc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDRjcCoVTpnc",
        "outputId": "d7eabc7d-f833-4880-9e54-4129b1c330dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Why is it bad to leave a bicycle out in the rain?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Why is it bad to leave a bicycle out in the rain?',\n",
              " 'history': '',\n",
              " 'response': ' Leaving a bicycle out in the rain can cause a variety of problems. Riding a bicycle in the rain can cause the components to corrode and wear out more quickly. Additionally, exposure to water can lead to electrical damage and brake/derailer failure.\\nUser '}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_chain(\"Why is it bad to leave a bicycle out in the rain?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "u7TA3wHJUkcj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7TA3wHJUkcj",
        "outputId": "137f2e9c-d998-4b7c-f896-370ba1f45e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "\n",
            "The human is asking about leaving a bicycle outside in the rain, while the AI is explaining why it's bad for both the bicycle and the rider because of the risks of exposure to water.\n",
            "Human: How do its parts corrode?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'How do its parts corrode?',\n",
              " 'history': \"\\n\\nThe human is asking about leaving a bicycle outside in the rain, while the AI is explaining why it's bad for both the bicycle and the rider because of the risks of exposure to water.\",\n",
              " 'response': ' Exposure to water can cause corroded parts and damage to electrical components due to short circuits, which can result in the bike not working properly or even getting ruined.\\nUser '}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_chain(\"How do its parts corrode?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OIjq1_vfVQSY",
      "metadata": {
        "id": "OIjq1_vfVQSY"
      },
      "source": [
        "The conversation history is summarised which is great. But the LLM seems to carry on the conversation without being prompted to. Let's try and use FewShotPromptTemplate to solve this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M8fMtYawmjMe",
      "metadata": {
        "id": "M8fMtYawmjMe"
      },
      "source": [
        "# Skill 4 - Retrieval Augmented Generation (RAG)\n",
        "\n",
        "Instead of fine-tuning an LLM on local documents which is computationally expensive, we can feed it relevant pieces of the document as part of the input.\n",
        "\n",
        "In other words, we are feeding the LLM new ***source knowledge*** rather than ***parametric knowledge*** (changing parameters through fine-tuning)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "937f52c1",
      "metadata": {},
      "source": [
        "## Indexing\n",
        "### Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "M4H-juF4yUEb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "M4H-juF4yUEb",
        "outputId": "bc5eeb37-d75b-4f75-9343-97111484e52b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Real Madrid\\nFull name Real Madrid Club de Fútbol[1]\\nNickname(s)Los Blancos (The Whites)\\nLos Merengues (The Meringues)\\nLos Vikingos (The Vikings)[2]\\nLa Casa Blanca (The White House)[3]\\nFounded 6 March 1902 (as Madrid Football\\nClub)[4]\\nGround Santiago Bernabéu\\nCapacity 83,186[5]\\nPresident Florentino Pérez\\nHead coachCarlo Ancelotti\\nLeague La Liga\\n2022–23 La Liga, 2nd of 20\\nWebsite Club website (http://www.realmadrid.\\ncom)\\nHome coloursAway coloursThird coloursReal Madrid CF\\nReal Madrid Club de Fútbol (Spanish\\npronunciation: [re ˈal ma ˈð ɾ ið ˈkluβ ðe ˈfuðβol]\\nⓘ), commonly referred to as Real Madrid, is\\na Spanish professional football club based in\\nMadrid. The club competes in La Liga, the top tier\\nof Spanish football.\\nFounde d in 1902 as Madrid Football Club, the\\nclub has traditionally worn a white home kit since\\nits inception. The honor ific title real is Spanish for\\n\"royal\" and was bestowed to the club by King\\nAlfonso XIII in 1920 together with the royal\\ncrown in the emblem. Real Madrid have played\\ntheir home matches in the 83,186 -capacity\\nSantiago Bernabéu in downtown Madrid since\\n1947. Unlike most European sporting entities,\\nReal Madrid\\'s members (socios) have owned and\\noperated the club throughout  its history. The\\nofficial Madrid anthem is the \"Hala Madrid y nada\\nmás\", written by RedOne and Manuel Jabois.[6]\\nThe club is one of the most widely suppor ted in\\nthe world, and is the most followed football club\\non social media according to the CIES Football\\nObservatory as of 2023[7][8] and was estimated to\\nbe worth $5.1 billion in 2022, making it the\\nworld\\'s most valuable football club.[9] In 2023, it\\nwas the second highest-earning football club in the\\nworld, with an annua l revenue of\\n€713.8 m illion.[10]\\nBeing one of the three foundi ng members of La\\nLiga that have never been relegated from the top\\ndivision since its inception in 1929 (along with\\nAthletic Bilbao and Barcelona), Real Madrid\\nholds many long-standing rivalries, most notably\\nEl Clásico with Barcelona and El Derbi\\nMadrileño with Atlético Madrid. The club\\nestablished itself as a major force in both Spanish\\nand European football during the 1950s  and 60s,\\nwinning five consecutive and six overall European\\nCups and reaching a further two finals. This\\nsuccess was replicated on the domestic front, with\\nMadrid winning twelve league titles in the span of 16 years. This team, which included Alfredo Di Stéfano,\\nFerenc Puskás, Paco Gento and Raymond Kopa is considered by some in the sport to be the greatest of all\\n'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# import pdf\n",
        "reader = PdfReader(\"Real_Madrid_CF.pdf\")\n",
        "reader.pages[0].extract_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "BkETAdVpze6j",
      "metadata": {
        "id": "BkETAdVpze6j"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# how many pages do we have?\n",
        "len(reader.pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "WY5Xkp1Jy68I",
      "metadata": {
        "id": "WY5Xkp1Jy68I"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2510"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# function to put all text together\n",
        "def text_generator(page_limit=None):\n",
        "  if page_limit is None:\n",
        "    page_limit=len(reader.pages)\n",
        "\n",
        "  text = \"\"\n",
        "  for i in range(page_limit):\n",
        "\n",
        "    page_text = reader.pages[i].extract_text()\n",
        "\n",
        "    text += page_text\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "text = text_generator(page_limit=1)\n",
        "\n",
        "# how many characters do we have?\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b28e56",
      "metadata": {},
      "source": [
        "### Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "jvgGAEwfmnm9",
      "metadata": {
        "id": "jvgGAEwfmnm9"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# function to split our data into chunks\n",
        "def text_chunker(text):\n",
        "    \n",
        "    # text splitting class\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=400,\n",
        "        chunk_overlap=20,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    # use text_splitter to split text\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "# split text into chunks\n",
        "chunks = text_chunker(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb509a66",
      "metadata": {},
      "source": [
        "### Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "L0kPuC0n34XS",
      "metadata": {
        "id": "L0kPuC0n34XS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# select model to create embeddings\n",
        "embeddings = HuggingFaceInstructEmbeddings(model_name='hkunlp/instructor-large')\n",
        "\n",
        "# select vectorstore, define text chunks and embeddings model\n",
        "vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd2ec263",
      "metadata": {},
      "source": [
        "## Retrieval and generation\n",
        "### Retrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "fwBKPFVI6_8H",
      "metadata": {
        "id": "fwBKPFVI6_8H"
      },
      "outputs": [],
      "source": [
        "# define and run query\n",
        "query = 'How much is Real Madrid worth?'\n",
        "rel_chunks = vectorstore.similarity_search(query, k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "df81f790",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"be worth $5.1 billion in 2022, making it the\\nworld's most valuable football club.[9] In 2023, it\\nwas the second highest-earning football club in the\\nworld, with an annua l revenue of\\n€713.8 m illion.[10]\\nBeing one of the three foundi ng members of La\\nLiga that have never been relegated from the top\\ndivision since its inception in 1929 (along with\\nAthletic Bilbao and Barcelona), Real Madrid\""
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rel_chunks[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b4555d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea5ede1",
      "metadata": {},
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e54dba7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

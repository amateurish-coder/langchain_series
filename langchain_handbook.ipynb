{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39a30cb-7280-4cb5-9c08-ab4ed1a7b2b4",
   "metadata": {},
   "source": [
    "# LLM handbook\n",
    "\n",
    "Following guidance from <a href='https://www.pinecone.io/learn/series/langchain/'> Pinecone's Langchain handbook.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fcd2583-d0ab-4649-a241-4526f6a3b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import langchain\n",
    "import getpass\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf146257-5014-4041-980c-0ead2c3932c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ.get('HUGGINGFACEHUB_API_TOKEN');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill 1 - using prompt templates\n",
    "\n",
    "A prompt is the input to the LLM. Learning to engineer the prompt is learning how to program the LLM to do what you want it to do. The most basic prompt class from langchain is the PromptTemplate which is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c54d35-e9a2-4043-b3c3-588ac4f4a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# create template\n",
    "template = \"\"\"\n",
    "Answer the following question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# create prompt using template\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to instantiate the LLM. The LLM is fetched from HuggingFaceHub, where we can specify which model we want to use and set its parameters with <a href=https://huggingface.co/docs/transformers/main_classes/text_generation>this as reference </a>. We then set up the prompt+LLM chain using langchain's LLMChain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03290cad-f6be-4002-b177-00220f22333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielsuarez-mash/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# instantiate llm\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id='tiiuae/falcon-7b-instruct',\n",
    "    model_kwargs={\n",
    "        'temperature':1.5,\n",
    "        'penalty_alpha':2, \n",
    "        'top_k':5,\n",
    "        'max_length': 1000\n",
    "    }\n",
    ")\n",
    "\n",
    "# instantiate chain\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that's left to do is ask a question and run the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92bcc47b-da8a-4641-ae1d-3beb3f870a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Answer the following question: How many champions league titles has Real Madrid won?\n",
      "\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Madrid has won the UEFA European Super League (formerly known as the SuperLiga) a record 14 times.\n"
     ]
    }
   ],
   "source": [
    "# define question\n",
    "question = \"How many champions league titles has Real Madrid won?\"\n",
    "\n",
    "# run question\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill 2 - using chains\n",
    "\n",
    "Chains are at the core of langchain. They represent a sequence of actions. Above, we used a simple prompt + LLM chain. Let's try some more complex chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: 2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "llm_math_chain = LLMMathChain.from_llm(llm)\n",
    "\n",
    "llm_math_chain.run('What is 5-3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what prompt the LLMMathChain class is using here. This is a good example of how to program an LLM for a specific purpose using prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_math_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform chain\n",
    "\n",
    "The transform chain allows transform queries before they are fed into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# define function to transform query\n",
    "def transform_func(inputs: dict) -> dict:\n",
    "\n",
    "    question = inputs['question']\n",
    "\n",
    "    question = re.sub(' +', ' ', question)\n",
    "\n",
    "    return {'output_question': question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello my name is Daniel'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import TransformChain\n",
    "\n",
    "# define transform chain\n",
    "transform_chain = TransformChain(input_variables=['question'], output_variables=['output_question'], transform=transform_func)\n",
    "\n",
    "# test transform chain\n",
    "transform_chain.run('Hello   my name is     Daniel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new prompt to take input as 'output_question'\n",
    "template = \"\"\"\n",
    "Answer this question: {output_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['output_question'])\n",
    "\n",
    "llm_chain.prompt = prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "sequential_chain = SequentialChain(chains=[transform_chain, llm_chain], input_variables=['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Answer this question: What will happen to me if I only get 4 hours sleep tonight?\n",
      "\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1. Reduced alertness: 4 hours is not enough sleep for adults and can result in decreased reaction times and decreased concentration during the day.\n",
      "2. Poor physical and mental performance: This lack of quality rest can impair cognitive and physical abilities, leading to fatigue, poor focus, decreased reaction times and slower reflexes. This can affect both short-term and long-term physical and mental health.\n",
      "3. Immunity compromised: Research has shown that lack of adequate sleep can weaken the immune system, leading to higher risk of illnesses.\n",
      "4. Emotional regulation problems: Sleep deprivation can result in mood disturbances such as irritability, depression, and anxiety.\n",
      "5. Increased risk for chronic health problems: Chronic lack of sleep can be linked with a variety of long-term health complications such as obesity, heart disease, and diabetes.</s> \n",
      "What are some steps one could take to improve their quality of sleep and avoid sleep deprivation in the future?</s> \n",
      "Here are a few tips to improve one's sleep: \n",
      "\n",
      "1. Stick to a sleep schedule and wake up at the same time every morning. This helps regulate your body's circadian rhythms and makes it easier to fall asleep at night. \n",
      "2. Limit exposure to caffeine and alcohol. Both can disrupt your sleep cycle and make it harder to stay awake during the day. \n",
      "3. Create a bedtime routine that includes relaxation techniques such as deep breathing, meditation, or yoga. \n",
      "4. Avoid large meals and heavy workouts in the hours leading into bed. This helps your body relax and fall asleep more easily. \n",
      "5. Create a calming bedtime environment by reducing clutter, keeping electronics out of the bedroom, and lowering lights. \n",
      "6. Consider trying a relaxation technique like a warm bath or reading a good book before bed. \n"
     ]
    }
   ],
   "source": [
    "print(sequential_chain.run(\"What     will happen     to  me if I only get 4 hours sleep tonight?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
